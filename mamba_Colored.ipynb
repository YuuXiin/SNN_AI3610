{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "train1 = torch.load(r'E:\\snn_final\\ColoredMNIST\\train1.pt',weights_only=False)\n",
    "train2 = torch.load(r'E:\\snn_final\\ColoredMNIST\\train2.pt',weights_only=False)\n",
    "test = torch.load(r'E:\\snn_final\\ColoredMNIST\\test.pt',weights_only=False)\n",
    "\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "def dataset_load(raw_dataset):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for one_data in raw_dataset:\n",
    "        y.append(one_data[1])\n",
    "        image=one_data[0]\n",
    "        data=np.array(image)\n",
    "        data=data.transpose(2,0,1)\n",
    "        x.append(data)\n",
    "    x=torch.Tensor(np.array(x))\n",
    "    y=torch.Tensor(np.array(y))\n",
    "    ds=torch.utils.data.TensorDataset(x,y)\n",
    "    return ds\n",
    "\n",
    "train1_data = dataset_load(train1)\n",
    "train2_data = dataset_load(train2)\n",
    "test_data = dataset_load(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import numpy as np  \n",
    "from spikingjelly.activation_based import neuron, encoding, functional, surrogate, layer  \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "class ReversalLayer(torch.autograd.Function):  \n",
    "    @staticmethod  \n",
    "    def forward(ctx, x, alpha):  \n",
    "        ctx.alpha = alpha  \n",
    "        return x.view_as(x)  \n",
    "\n",
    "    @staticmethod  \n",
    "    def backward(ctx, grad_output):  \n",
    "        output = grad_output.neg() * ctx.alpha  \n",
    "        return output, None  \n",
    "\n",
    "class SpikingDANN_mamba(nn.Module):  \n",
    "    def __init__(self):  \n",
    "        super(SpikingDANN_mamba, self).__init__()  \n",
    "        \n",
    "        # 主路径是一个基础的卷积网络，提取初始特征\n",
    "        self.main_branch = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),  \n",
    "            nn.MaxPool2d(2, 2),  \n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # 分支路径有不同尺度的卷积核，用于提取多尺度特征\n",
    "        self.branch1 = nn.Sequential(  # 分支1，卷积核大小为3x3  \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),  \n",
    "            nn.BatchNorm2d(32),  \n",
    "            nn.MaxPool2d(2, 2),  \n",
    "            nn.ReLU(inplace=True),  \n",
    "        )  \n",
    "\n",
    "        self.branch2 = nn.Sequential(  # 分支2，卷积核大小为5x5  \n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),  \n",
    "            nn.BatchNorm2d(32),  \n",
    "            nn.MaxPool2d(2, 2),  \n",
    "            nn.ReLU(inplace=True),  \n",
    "        )  \n",
    "\n",
    "        self.branch3 = nn.Sequential(  # 分支3，卷积核大小为7x7  \n",
    "            nn.Conv2d(16, 32, kernel_size=7, padding=3),  \n",
    "            nn.BatchNorm2d(32),  \n",
    "            nn.MaxPool2d(2, 2),  \n",
    "            nn.ReLU(inplace=True),  \n",
    "        )  \n",
    "\n",
    "        # 通过线性层和softmax实现动态加权融合，根据输入的统计信息为每个分支赋权\n",
    "        self.fusion_weights = nn.Sequential(  \n",
    "            nn.Linear(3, 3),  # 分支数为3  \n",
    "            nn.Softmax(dim=1)  # 动态加权融合分数  \n",
    "        ) \n",
    "\n",
    "        self.feature_fusion = nn.Conv2d(32 * 3, 16, kernel_size=1)  # 将3个分支拼接  \n",
    "        \n",
    "        # SNN Classifier with Leaky Integrate-and-Fire neurons  \n",
    "        self.classifier = nn.Sequential(  \n",
    "            nn.Linear(784, 32),  \n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan()),  \n",
    "            nn.Linear(32, 32),  \n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan()),  \n",
    "            nn.Linear(32, 2),  \n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan()),  \n",
    "        )  \n",
    "        \n",
    "        # Domain Discriminator  \n",
    "        self.domain_discriminator = nn.Sequential(  \n",
    "            nn.Linear(784, 16),  \n",
    "            nn.Dropout(0.3),\n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan()),  \n",
    "            nn.Linear(16, 2), \n",
    "        )  \n",
    "\n",
    "    def forward(self, input_spikes, alpha=1.0):  \n",
    "        main_features = self.main_branch(input_spikes)\n",
    "        branch1_features = self.branch1(main_features)\n",
    "        branch2_features = self.branch2(main_features)\n",
    "        branch3_features = self.branch3(main_features)\n",
    "        branch_outputs = torch.stack([\n",
    "            branch1_features.mean(dim=(2, 3)),  # 平均池化 -> (B, C)  \n",
    "            branch2_features.mean(dim=(2, 3)),   \n",
    "            branch3_features.mean(dim=(2, 3)) \n",
    "        ], dim=-1)\n",
    "\n",
    "        fusion_weights = self.fusion_weights(branch_outputs.mean(dim=1))\n",
    "        weighted_branch1 = fusion_weights[:, 0].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * branch1_features  \n",
    "        weighted_branch2 = fusion_weights[:, 1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * branch2_features  \n",
    "        weighted_branch3 = fusion_weights[:, 2].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * branch3_features  \n",
    "        fused_features = torch.cat([weighted_branch1, weighted_branch2, weighted_branch3], dim=1)  \n",
    "\n",
    "        features = self.feature_fusion(fused_features)  \n",
    "        flattened_features = features.view(input_spikes.shape[0], -1)\n",
    "\n",
    "        class_output = self.classifier(flattened_features)  \n",
    "        domain_output = self.domain_discriminator(ReversalLayer.apply(flattened_features, alpha))  \n",
    "        \n",
    "        return class_output, domain_output  \n",
    "      \n",
    "def evaluate_accuracy(data_iter, net, encoder, T):  \n",
    "    acc_sum, n = 0.0, 0\n",
    "    net.eval()  \n",
    "    with torch.no_grad():  \n",
    "        for X, y in data_iter:   \n",
    "            for t in range(T):\n",
    "                spike_input = encoder(X)  # Encode to spikes  \n",
    "                class_output, _ = net(spike_input)  \n",
    "                if t == 0:\n",
    "                    out_fr = torch.zeros_like(class_output) \n",
    "                out_fr += class_output\n",
    "            out_fr = out_fr / T\n",
    "            acc_sum += (out_fr.argmax(1) == y).float().sum().item()  \n",
    "            n += y.numel()\n",
    "            functional.reset_net(net)\n",
    "    return acc_sum / n \n",
    "    \n",
    "\n",
    "def train_snn(net, train1_data, train2_data, test_data):  \n",
    "    class parser:  \n",
    "        def __init__(self):  \n",
    "            self.T = 5  \n",
    "            self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'  \n",
    "            self.epochs = 10  \n",
    "            self.b = 128  \n",
    "            self.j = 4  \n",
    "            self.out_dir = './logs'  \n",
    "            self.resume = None\n",
    "            self.amp = True  \n",
    "            self.opt = 'adam'  \n",
    "            self.lr = 1e-3  \n",
    "            self.tau = 2.0  \n",
    "            self.alpha_domain = 1.0\n",
    "    \n",
    "    args = parser()\n",
    "    \n",
    "    encoder = encoding.PoissonEncoder()\n",
    "    loss_class_func = nn.CrossEntropyLoss()\n",
    "    loss_domain_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=args.lr)\n",
    "\n",
    "    out_dir = os.path.join(  \n",
    "        args.out_dir,   \n",
    "        f'Mamba_DANN_SNN_T{args.T}'  \n",
    "    )  \n",
    "    os.makedirs(out_dir, exist_ok=True)  \n",
    "\n",
    "    writer = SummaryWriter(log_dir=out_dir)\n",
    "    \n",
    "    for epoch in range(args.epochs):  \n",
    "        net.train()\n",
    "        train1_acc, train2_acc, class_loss, domain1_loss, domain2_loss = 0,0,0,0,0 \n",
    "        domain1_acc, domain2_acc = 0,0\n",
    "        train_samples = 0\n",
    "        p = float(epoch)/float(args.epochs)\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1  \n",
    "        for (source_img, source_class), (target_img, target_class) in zip(train1_data, train2_data):  \n",
    "            optimizer.zero_grad()  \n",
    "            source_spikes = encoder(source_img) # 128,3,28,28\n",
    "            target_spikes = encoder(target_img)   \n",
    "            source_domain = torch.zeros(source_img.shape[0],dtype=torch.long).to(source_img.device)  \n",
    "            target_domain = torch.ones(target_img.shape[0],dtype=torch.long).to(target_img.device)  \n",
    "            source_class = source_class.long()\n",
    "            target_class = target_class.long()\n",
    " \n",
    "            for t in range(args.T):\n",
    "                class_output_source, domain_output_source = net(source_spikes,alpha=alpha)  \n",
    "                class_output_target, domain_output_target = net(target_spikes,alpha=alpha)  \n",
    "                \n",
    "                if t == 0:\n",
    "                    out_fr_class_s = torch.zeros_like(class_output_source)  \n",
    "                    out_fr_domain_s = torch.zeros_like(domain_output_source)  \n",
    "                    out_fr_class_t = torch.zeros_like(class_output_target)  \n",
    "                    out_fr_domain_t = torch.zeros_like(domain_output_target) \n",
    "                \n",
    "                out_fr_class_s += class_output_source\n",
    "                out_fr_domain_s += domain_output_source\n",
    "                out_fr_class_t += class_output_target\n",
    "                out_fr_domain_t += domain_output_target\n",
    "            out_fr_class_s = out_fr_class_s / args.T\n",
    "            out_fr_domain_s = out_fr_domain_s / args.T\n",
    "            out_fr_class_t = out_fr_class_t / args.T\n",
    "            out_fr_domain_t = out_fr_domain_t / args.T\n",
    "\n",
    "            # Calculate losses  \n",
    "            loss_class = loss_class_func(out_fr_class_s, source_class)  \n",
    "            loss_domain_source = loss_domain_func(out_fr_domain_s, source_domain)  \n",
    "            loss_domain_target = loss_domain_func(out_fr_domain_t, target_domain)   \n",
    "            loss = loss_class + loss_domain_source + loss_domain_target  \n",
    "            \n",
    "            loss.backward()  # Backpropagation  \n",
    "            optimizer.step()  # Optimization  \n",
    "\n",
    "            train_samples += source_class.size(0)\n",
    "            class_loss += loss_class.item() * source_class.size(0)\n",
    "            domain1_loss += loss_domain_source.item() * source_class.size(0)\n",
    "            domain2_loss += loss_domain_target.item() * source_class.size(0)\n",
    "            train1_acc += (out_fr_class_s.argmax(1) == source_class).float().sum().item()\n",
    "            train2_acc += (out_fr_class_t.argmax(1) == target_class).float().sum().item()\n",
    "            domain1_acc += (out_fr_domain_s.argmax(1) == source_domain).float().sum().item()\n",
    "            domain2_acc += (out_fr_domain_t.argmax(1) == target_domain).float().sum().item()\n",
    "\n",
    "            functional.reset_net(net)\n",
    "\n",
    "        class_loss /= train_samples\n",
    "        domain1_loss /= train_samples\n",
    "        domain2_loss /= train_samples\n",
    "        train1_acc /= train_samples\n",
    "        train2_acc /= train_samples\n",
    "        domain1_acc /= train_samples\n",
    "        domain2_acc /= train_samples\n",
    "\n",
    "        acc = evaluate_accuracy(test_data, net, encoder, args.T)\n",
    "\n",
    "        writer.add_scalar('Class Acc on source', train1_acc, epoch)\n",
    "        writer.add_scalar('Class Acc on target', train2_acc, epoch)\n",
    "        writer.add_scalar('Domain Acc on source', domain1_acc, epoch)\n",
    "        writer.add_scalar('Domain Acc on target', domain2_acc, epoch)\n",
    "        writer.add_scalar('Class Loss', class_loss, epoch)\n",
    "        writer.add_scalar('Domain Loss on source', domain1_loss, epoch)\n",
    "        writer.add_scalar('Domain Loss on target', domain2_loss, epoch)\n",
    "        writer.add_scalar('Class Acc on test', acc, epoch)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}, Class Loss: {class_loss:.4f}, Domain Source Loss: {domain1_loss:.4f}, Domain Target Loss: {domain2_loss:.4f}')   \n",
    "        print(f'Class Acc on train_source: {train1_acc:.4f}, Domain Acc on train_source: {domain1_acc:.4f}')  \n",
    "        print(f'Class Acc on train_target: {train2_acc:.4f}, Domain Acc on train_target: {domain2_acc:.4f}')  \n",
    "        print(f'Class Acc on test: {acc:.4f}')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Class Loss: 0.6467, Domain Source Loss: 0.6799, Domain Target Loss: 0.6138\n",
      "Class Acc on train_source: 0.5393, Domain Acc on train_source: 0.5115\n",
      "Class Acc on train_target: 0.6025, Domain Acc on train_target: 0.7668\n",
      "Class Acc on test: 0.0997\n",
      "Epoch 2, Class Loss: 0.5149, Domain Source Loss: 0.6627, Domain Target Loss: 0.6425\n",
      "Class Acc on train_source: 0.7981, Domain Acc on train_source: 0.6270\n",
      "Class Acc on train_target: 0.8972, Domain Acc on train_target: 0.6152\n",
      "Class Acc on test: 0.0998\n",
      "Epoch 3, Class Loss: 0.5166, Domain Source Loss: 0.6797, Domain Target Loss: 0.6803\n",
      "Class Acc on train_source: 0.7950, Domain Acc on train_source: 0.5864\n",
      "Class Acc on train_target: 0.8895, Domain Acc on train_target: 0.5575\n",
      "Class Acc on test: 0.0998\n",
      "Epoch 4, Class Loss: 0.5167, Domain Source Loss: 0.6986, Domain Target Loss: 0.6867\n",
      "Class Acc on train_source: 0.7945, Domain Acc on train_source: 0.5050\n",
      "Class Acc on train_target: 0.8893, Domain Acc on train_target: 0.5610\n",
      "Class Acc on test: 0.1124\n",
      "Epoch 5, Class Loss: 0.5136, Domain Source Loss: 0.6903, Domain Target Loss: 0.6895\n",
      "Class Acc on train_source: 0.7978, Domain Acc on train_source: 0.5379\n",
      "Class Acc on train_target: 0.8921, Domain Acc on train_target: 0.5303\n",
      "Class Acc on test: 0.0997\n",
      "Epoch 6, Class Loss: 0.5159, Domain Source Loss: 0.6914, Domain Target Loss: 0.6925\n",
      "Class Acc on train_source: 0.7963, Domain Acc on train_source: 0.5119\n",
      "Class Acc on train_target: 0.8931, Domain Acc on train_target: 0.5461\n",
      "Class Acc on test: 0.1162\n",
      "Epoch 7, Class Loss: 0.5205, Domain Source Loss: 0.6862, Domain Target Loss: 0.6974\n",
      "Class Acc on train_source: 0.7903, Domain Acc on train_source: 0.5322\n",
      "Class Acc on train_target: 0.8820, Domain Acc on train_target: 0.5242\n",
      "Class Acc on test: 0.1038\n",
      "Epoch 8, Class Loss: 0.5164, Domain Source Loss: 0.6833, Domain Target Loss: 0.6970\n",
      "Class Acc on train_source: 0.7957, Domain Acc on train_source: 0.5201\n",
      "Class Acc on train_target: 0.8926, Domain Acc on train_target: 0.5505\n",
      "Class Acc on test: 0.1061\n",
      "Epoch 9, Class Loss: 0.5165, Domain Source Loss: 0.6869, Domain Target Loss: 0.6940\n",
      "Class Acc on train_source: 0.7954, Domain Acc on train_source: 0.4965\n",
      "Class Acc on train_target: 0.8861, Domain Acc on train_target: 0.5593\n",
      "Class Acc on test: 0.1058\n",
      "Epoch 10, Class Loss: 0.5148, Domain Source Loss: 0.6890, Domain Target Loss: 0.6928\n",
      "Class Acc on train_source: 0.7970, Domain Acc on train_source: 0.5062\n",
      "Class Acc on train_target: 0.8921, Domain Acc on train_target: 0.5535\n",
      "Class Acc on test: 0.1017\n"
     ]
    }
   ],
   "source": [
    "net = SpikingDANN_mamba() \n",
    "  \n",
    "train1_loader = torch.utils.data.DataLoader(train1_data, batch_size=128, shuffle=True)\n",
    "train2_loader = torch.utils.data.DataLoader(train2_data, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=True)\n",
    " \n",
    "train_snn(net, train1_loader, train2_loader, test_loader)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
