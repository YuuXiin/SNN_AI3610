{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.cuda import amp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "from spikingjelly.activation_based import neuron, encoding, functional, surrogate, layer\n",
    "\n",
    "import simple_snn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # 只显示error，不显示其他信息\n",
    "\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self, tau):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            layer.Flatten(), # 输入展开为一维张量\n",
    "            layer.Linear(28 * 28, 128, bias=False), # 28*28个输入映射到10个输出\n",
    "            neuron.LIFNode(tau=tau, surrogate_function=surrogate.ATan()),\n",
    "            layer.Linear(128, 10, bias=False),\n",
    "            neuron.LIFNode(tau=tau, surrogate_function=surrogate.ATan()),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer(x)\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    :return: None\n",
    "\n",
    "    * :ref:`API in English <lif_fc_mnist.main-en>`\n",
    "\n",
    "    .. _lif_fc_mnist.main-cn:\n",
    "\n",
    "    使用全连接-LIF的网络结构，进行MNIST识别。\\n\n",
    "    这个函数会初始化网络进行训练，并显示训练过程中在测试集的正确率。\n",
    "\n",
    "    * :ref:`中文API <lif_fc_mnist.main-cn>`\n",
    "\n",
    "    .. _lif_fc_mnist.main-en:\n",
    "\n",
    "    The network with FC-LIF structure for classifying MNIST.\\n\n",
    "    This function initials the network, starts trainingand shows accuracy on test dataset.\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(description='LIF MNIST Training')\n",
    "    parser.add_argument('-T', default=100, type=int, help='simulating time-steps') # 脉冲时间步长，默认100\n",
    "    parser.add_argument('-device', default='cpu', help='device')\n",
    "    parser.add_argument('-b', default=64, type=int, help='batch size')\n",
    "    parser.add_argument('-epochs', default=10, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('-j', default=4, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    parser.add_argument('-data-dir', type=str, help='root dir of MNIST dataset')\n",
    "    parser.add_argument('-out-dir', type=str, default='./logs', help='root dir for saving logs and checkpoint')\n",
    "    parser.add_argument('-resume', type=str, help='resume from the checkpoint path')\n",
    "    parser.add_argument('-amp', action='store_true', help='automatic mixed precision training')\n",
    "    parser.add_argument('-opt', type=str, choices=['sgd', 'adam'], default='adam', help='use which optimizer. SGD or Adam')\n",
    "    parser.add_argument('-momentum', default=0.9, type=float, help='momentum for SGD')\n",
    "    parser.add_argument('-lr', default=1e-3, type=float, help='learning rate')\n",
    "    parser.add_argument('-tau', default=2.0, type=float, help='parameter tau of LIF neuron')\n",
    "\n",
    "    args, unknown = parser.parse_known_args(sys.argv[1:])\n",
    "    # args = parser.parse_args()\n",
    "    print(args)\n",
    "\n",
    "    net = SNN(tau=args.tau)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    net.to(args.device)\n",
    "\n",
    "    # 初始化数据加载器\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root='./mnist_data',\n",
    "        train=True,\n",
    "        transform=torchvision.transforms.ToTensor(),\n",
    "        download=True\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root='./mnist_data',\n",
    "        train=False,\n",
    "        transform=torchvision.transforms.ToTensor(),\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    train_data_loader = data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=args.b, # 默认是64\n",
    "        shuffle=True,\n",
    "        drop_last=True, # 如果数据集大小不能被batch size整除，则丢弃最后一个batch\n",
    "        num_workers=args.j, # 子进程数，默认是4\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_data_loader = data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=args.b,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=args.j,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    scaler = None\n",
    "    if args.amp:\n",
    "        scaler = amp.GradScaler()\n",
    "\n",
    "    start_epoch = 0\n",
    "    max_test_acc = -1\n",
    "\n",
    "    optimizer = None\n",
    "    # 默认使用Adam优化器\n",
    "    if args.opt == 'sgd':\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    elif args.opt == 'adam':\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "    else:\n",
    "        raise NotImplementedError(args.opt)\n",
    "\n",
    "    # 是否启用断点续训\n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        max_test_acc = checkpoint['max_test_acc']\n",
    "    \n",
    "    out_dir = os.path.join(args.out_dir, f'T{args.T}_b{args.b}_{args.opt}_lr{args.lr}')\n",
    "\n",
    "    if torch.cuda.is_available() and args.amp:\n",
    "        out_dir += '_amp'\n",
    "\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "        print(f'Mkdir {out_dir}.')\n",
    "\n",
    "    with open(os.path.join(out_dir, 'args.txt'), 'w', encoding='utf-8') as args_txt:\n",
    "        args_txt.write(str(args))\n",
    "\n",
    "    # writer = SummaryWriter(out_dir, purge_step=start_epoch)\n",
    "    with open(os.path.join(out_dir, 'args.txt'), 'w', encoding='utf-8') as args_txt:\n",
    "        args_txt.write(str(args))\n",
    "        args_txt.write('\\n')\n",
    "        args_txt.write(' '.join(sys.argv))\n",
    "\n",
    "    encoder = encoding.PoissonEncoder() # 泊松分布决定是否释放脉冲，得到64*1*28*28\n",
    "\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        start_time = time.time()\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        train_samples = 0\n",
    "        for img, label in train_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            img = img.to(args.device)\n",
    "            label = label.to(args.device)\n",
    "            label_onehot = F.one_hot(label, 10).float() # 将label转换为10位one-hot编码\n",
    "\n",
    "            if scaler is not None: # 是否使用自动混合精度cuda\n",
    "                with amp.autocast():\n",
    "                    out_fr = 0.\n",
    "                    for t in range(args.T):\n",
    "                        encoded_img = encoder(img)\n",
    "                        out_fr += net(encoded_img)\n",
    "                    out_fr = out_fr / args.T\n",
    "                    loss = F.mse_loss(out_fr, label_onehot)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else: # 没有gpu加速，不使用\n",
    "                out_fr = 0.\n",
    "                for t in range(args.T):\n",
    "                    encoded_img = encoder(img) # 64*1*28*28\n",
    "                    res = net(encoded_img) # 64*10\n",
    "                    out_fr += res # 传入snn网络，得到所有时间步的累积输出\n",
    "\n",
    "                out_fr = out_fr / args.T # 对所有时间步取平均\n",
    "                loss = F.mse_loss(out_fr, label_onehot)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_samples += label.numel()\n",
    "            train_loss += loss.item() * label.numel()\n",
    "            train_acc += (out_fr.argmax(1) == label).float().sum().item() #取概率最大值的索引，与label比较，得到正确率\n",
    "\n",
    "            functional.reset_net(net)\n",
    "\n",
    "        train_time = time.time()\n",
    "        train_speed = train_samples / (train_time - start_time)\n",
    "        train_loss /= train_samples\n",
    "        train_acc /= train_samples\n",
    "\n",
    "        # writer.add_scalar('train_loss', train_loss, epoch)\n",
    "        # writer.add_scalar('train_acc', train_acc, epoch)\n",
    "\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        test_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for img, label in test_data_loader:\n",
    "                img = img.to(args.device)\n",
    "                label = label.to(args.device)\n",
    "                label_onehot = F.one_hot(label, 10).float()\n",
    "                out_fr = 0.\n",
    "                for t in range(args.T):\n",
    "                    encoded_img = encoder(img)\n",
    "                    out_fr += net(encoded_img)\n",
    "                out_fr = out_fr / args.T\n",
    "                loss = F.mse_loss(out_fr, label_onehot)\n",
    "\n",
    "                test_samples += label.numel()\n",
    "                test_loss += loss.item() * label.numel()\n",
    "                test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "                functional.reset_net(net)\n",
    "        test_time = time.time()\n",
    "        test_speed = test_samples / (test_time - train_time)\n",
    "        test_loss /= test_samples\n",
    "        test_acc /= test_samples\n",
    "        # writer.add_scalar('test_loss', test_loss, epoch)\n",
    "        # writer.add_scalar('test_acc', test_acc, epoch)\n",
    "\n",
    "        save_max = False\n",
    "        if test_acc > max_test_acc:\n",
    "            max_test_acc = test_acc\n",
    "            save_max = True\n",
    "\n",
    "        checkpoint = {\n",
    "            'net': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'max_test_acc': max_test_acc\n",
    "        }\n",
    "\n",
    "        if save_max:\n",
    "            torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_max.pth'))\n",
    "\n",
    "        torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_latest.pth'))\n",
    "\n",
    "        print(args)\n",
    "        print(out_dir)\n",
    "        print(f'epoch ={epoch}, train_loss ={train_loss: .4f}, train_acc ={train_acc: .4f}, test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}, max_test_acc ={max_test_acc: .4f}')\n",
    "        print(f'train speed ={train_speed: .4f} images/s, test speed ={test_speed: .4f} images/s')\n",
    "        print(f'escape time = {(datetime.datetime.now() + datetime.timedelta(seconds=(time.time() - start_time) * (args.epochs - epoch))).strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "\n",
    "    # 保存绘图用数据\n",
    "    net.eval()\n",
    "    # 注册钩子\n",
    "    output_layer = net.layer[-1] # 输出层\n",
    "    output_layer.v_seq = []\n",
    "    output_layer.s_seq = []\n",
    "    def save_hook(m, x, y):\n",
    "        m.v_seq.append(m.v.unsqueeze(0))\n",
    "        m.s_seq.append(y.unsqueeze(0))\n",
    "\n",
    "    output_layer.register_forward_hook(save_hook)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img, label = test_dataset[0]\n",
    "        img = img.to(args.device)\n",
    "        out_fr = 0.\n",
    "        for t in range(args.T):\n",
    "            encoded_img = encoder(img)\n",
    "            out_fr += net(encoded_img)\n",
    "        out_spikes_counter_frequency = (out_fr / args.T).cpu().numpy()\n",
    "        print(f'Firing rate: {out_spikes_counter_frequency}')\n",
    "\n",
    "        output_layer.v_seq = torch.cat(output_layer.v_seq)\n",
    "        output_layer.s_seq = torch.cat(output_layer.s_seq)\n",
    "        v_t_array = output_layer.v_seq.cpu().numpy().squeeze()  # v_t_array[i][j]表示神经元i在j时刻的电压值\n",
    "        np.save(\"v_t_array.npy\",v_t_array)\n",
    "        s_t_array = output_layer.s_seq.cpu().numpy().squeeze()  # s_t_array[i][j]表示神经元i在j时刻释放的脉冲，为0或1\n",
    "        np.save(\"s_t_array.npy\",s_t_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(T=100, device='cpu', b=64, epochs=10, j=4, data_dir=None, out_dir='./logs', resume=None, amp=False, opt='adam', momentum=0.9, lr=0.001, tau=2.0)\n",
      "SNN(\n",
      "  (layer): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1, step_mode=s)\n",
      "    (1): Linear(in_features=784, out_features=128, bias=False)\n",
      "    (2): LIFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=s, backend=torch, tau=2.0\n",
      "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "    )\n",
      "    (3): Linear(in_features=128, out_features=10, bias=False)\n",
      "    (4): LIFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=s, backend=torch, tau=2.0\n",
      "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Namespace(T=100, device='cpu', b=64, epochs=10, j=4, data_dir=None, out_dir='./logs', resume=None, amp=False, opt='adam', momentum=0.9, lr=0.001, tau=2.0)\n",
      "./logs\\T100_b64_adam_lr0.001\n",
      "epoch =0, train_loss = 0.0200, train_acc = 0.8910, test_loss = 0.0102, test_acc = 0.9451, max_test_acc = 0.9451\n",
      "train speed = 446.8080 images/s, test speed = 680.5378 images/s\n",
      "escape time = 2025-01-04 15:38:18\n",
      "\n",
      "Namespace(T=100, device='cpu', b=64, epochs=10, j=4, data_dir=None, out_dir='./logs', resume=None, amp=False, opt='adam', momentum=0.9, lr=0.001, tau=2.0)\n",
      "./logs\\T100_b64_adam_lr0.001\n",
      "epoch =1, train_loss = 0.0088, train_acc = 0.9538, test_loss = 0.0075, test_acc = 0.9610, max_test_acc = 0.9610\n",
      "train speed = 339.0937 images/s, test speed = 612.2358 images/s\n",
      "escape time = 2025-01-04 15:45:41\n",
      "\n",
      "Namespace(T=100, device='cpu', b=64, epochs=10, j=4, data_dir=None, out_dir='./logs', resume=None, amp=False, opt='adam', momentum=0.9, lr=0.001, tau=2.0)\n",
      "./logs\\T100_b64_adam_lr0.001\n",
      "epoch =2, train_loss = 0.0066, train_acc = 0.9663, test_loss = 0.0063, test_acc = 0.9672, max_test_acc = 0.9672\n",
      "train speed = 505.3324 images/s, test speed = 938.1461 images/s\n",
      "escape time = 2025-01-04 15:36:06\n",
      "\n",
      "Namespace(T=100, device='cpu', b=64, epochs=10, j=4, data_dir=None, out_dir='./logs', resume=None, amp=False, opt='adam', momentum=0.9, lr=0.001, tau=2.0)\n",
      "./logs\\T100_b64_adam_lr0.001\n",
      "epoch =3, train_loss = 0.0053, train_acc = 0.9734, test_loss = 0.0056, test_acc = 0.9715, max_test_acc = 0.9715\n",
      "train speed = 351.6127 images/s, test speed = 623.1354 images/s\n",
      "escape time = 2025-01-04 15:43:44\n",
      "\n",
      "Namespace(T=100, device='cpu', b=64, epochs=10, j=4, data_dir=None, out_dir='./logs', resume=None, amp=False, opt='adam', momentum=0.9, lr=0.001, tau=2.0)\n",
      "./logs\\T100_b64_adam_lr0.001\n",
      "epoch =4, train_loss = 0.0045, train_acc = 0.9785, test_loss = 0.0050, test_acc = 0.9735, max_test_acc = 0.9735\n",
      "train speed = 572.3445 images/s, test speed = 990.2898 images/s\n",
      "escape time = 2025-01-04 15:35:22\n",
      "\n",
      "Namespace(T=100, device='cpu', b=64, epochs=10, j=4, data_dir=None, out_dir='./logs', resume=None, amp=False, opt='adam', momentum=0.9, lr=0.001, tau=2.0)\n",
      "./logs\\T100_b64_adam_lr0.001\n",
      "epoch =5, train_loss = 0.0039, train_acc = 0.9821, test_loss = 0.0048, test_acc = 0.9739, max_test_acc = 0.9739\n",
      "train speed = 609.0351 images/s, test speed = 979.4226 images/s\n",
      "escape time = 2025-01-04 15:34:45\n",
      "\n",
      "Namespace(T=100, device='cpu', b=64, epochs=10, j=4, data_dir=None, out_dir='./logs', resume=None, amp=False, opt='adam', momentum=0.9, lr=0.001, tau=2.0)\n",
      "./logs\\T100_b64_adam_lr0.001\n",
      "epoch =6, train_loss = 0.0035, train_acc = 0.9851, test_loss = 0.0046, test_acc = 0.9762, max_test_acc = 0.9762\n",
      "train speed = 606.4108 images/s, test speed = 979.4725 images/s\n",
      "escape time = 2025-01-04 15:34:47\n",
      "\n",
      "Namespace(T=100, device='cpu', b=64, epochs=10, j=4, data_dir=None, out_dir='./logs', resume=None, amp=False, opt='adam', momentum=0.9, lr=0.001, tau=2.0)\n",
      "./logs\\T100_b64_adam_lr0.001\n",
      "epoch =7, train_loss = 0.0031, train_acc = 0.9868, test_loss = 0.0044, test_acc = 0.9768, max_test_acc = 0.9768\n",
      "train speed = 515.6889 images/s, test speed = 611.9304 images/s\n",
      "escape time = 2025-01-04 15:36:21\n",
      "\n",
      "Namespace(T=100, device='cpu', b=64, epochs=10, j=4, data_dir=None, out_dir='./logs', resume=None, amp=False, opt='adam', momentum=0.9, lr=0.001, tau=2.0)\n",
      "./logs\\T100_b64_adam_lr0.001\n",
      "epoch =8, train_loss = 0.0028, train_acc = 0.9885, test_loss = 0.0043, test_acc = 0.9765, max_test_acc = 0.9768\n",
      "train speed = 464.7386 images/s, test speed = 986.5223 images/s\n",
      "escape time = 2025-01-04 15:36:41\n",
      "\n",
      "Namespace(T=100, device='cpu', b=64, epochs=10, j=4, data_dir=None, out_dir='./logs', resume=None, amp=False, opt='adam', momentum=0.9, lr=0.001, tau=2.0)\n",
      "./logs\\T100_b64_adam_lr0.001\n",
      "epoch =9, train_loss = 0.0026, train_acc = 0.9900, test_loss = 0.0041, test_acc = 0.9767, max_test_acc = 0.9768\n",
      "train speed = 613.0745 images/s, test speed = 979.7963 images/s\n",
      "escape time = 2025-01-04 15:35:39\n",
      "\n",
      "Firing rate: [[0.   0.   0.   0.   0.   0.   0.   0.99 0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
