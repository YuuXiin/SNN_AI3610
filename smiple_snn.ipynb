{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.cuda import amp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "from spikingjelly.activation_based import neuron, encoding, functional, surrogate, layer\n",
    "\n",
    "import simple_snn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # 只显示error，不显示其他信息\n",
    "\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self, tau):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            layer.Flatten(), # 输入展开为一维张量\n",
    "            layer.Linear(28 * 28, 128, bias=False), # 28*28个输入映射到10个输出\n",
    "            neuron.LIFNode(tau=tau, surrogate_function=surrogate.ATan()),\n",
    "            layer.Linear(128, 10, bias=False),\n",
    "            neuron.LIFNode(tau=tau, surrogate_function=surrogate.ATan()),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer(x)\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    :return: None\n",
    "\n",
    "    * :ref:`API in English <lif_fc_mnist.main-en>`\n",
    "\n",
    "    .. _lif_fc_mnist.main-cn:\n",
    "\n",
    "    使用全连接-LIF的网络结构，进行MNIST识别。\\n\n",
    "    这个函数会初始化网络进行训练，并显示训练过程中在测试集的正确率。\n",
    "\n",
    "    * :ref:`中文API <lif_fc_mnist.main-cn>`\n",
    "\n",
    "    .. _lif_fc_mnist.main-en:\n",
    "\n",
    "    The network with FC-LIF structure for classifying MNIST.\\n\n",
    "    This function initials the network, starts trainingand shows accuracy on test dataset.\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(description='LIF MNIST Training')\n",
    "    parser.add_argument('-T', default=100, type=int, help='simulating time-steps') # 脉冲时间步长，默认100\n",
    "    parser.add_argument('-device', default='cpu', help='device')\n",
    "    parser.add_argument('-b', default=64, type=int, help='batch size')\n",
    "    parser.add_argument('-epochs', default=100, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('-j', default=4, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    parser.add_argument('-data-dir', type=str, help='root dir of MNIST dataset')\n",
    "    parser.add_argument('-out-dir', type=str, default='./logs', help='root dir for saving logs and checkpoint')\n",
    "    parser.add_argument('-resume', type=str, help='resume from the checkpoint path')\n",
    "    parser.add_argument('-amp', action='store_true', help='automatic mixed precision training')\n",
    "    parser.add_argument('-opt', type=str, choices=['sgd', 'adam'], default='adam', help='use which optimizer. SGD or Adam')\n",
    "    parser.add_argument('-momentum', default=0.9, type=float, help='momentum for SGD')\n",
    "    parser.add_argument('-lr', default=1e-3, type=float, help='learning rate')\n",
    "    parser.add_argument('-tau', default=2.0, type=float, help='parameter tau of LIF neuron')\n",
    "\n",
    "    args, unknown = parser.parse_known_args(sys.argv[1:])\n",
    "    # args = parser.parse_args()\n",
    "    print(args)\n",
    "\n",
    "    net = SNN(tau=args.tau)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    net.to(args.device)\n",
    "\n",
    "    # 初始化数据加载器\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root='./mnist_data',\n",
    "        train=True,\n",
    "        transform=torchvision.transforms.ToTensor(),\n",
    "        download=True\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root='./mnist_data',\n",
    "        train=False,\n",
    "        transform=torchvision.transforms.ToTensor(),\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    train_data_loader = data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=args.b, # 默认是64\n",
    "        shuffle=True,\n",
    "        drop_last=True, # 如果数据集大小不能被batch size整除，则丢弃最后一个batch\n",
    "        num_workers=args.j, # 子进程数，默认是4\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_data_loader = data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=args.b,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=args.j,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    scaler = None\n",
    "    if args.amp:\n",
    "        scaler = amp.GradScaler()\n",
    "\n",
    "    start_epoch = 0\n",
    "    max_test_acc = -1\n",
    "\n",
    "    optimizer = None\n",
    "    # 默认使用Adam优化器\n",
    "    if args.opt == 'sgd':\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    elif args.opt == 'adam':\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "    else:\n",
    "        raise NotImplementedError(args.opt)\n",
    "\n",
    "    # 是否启用断点续训\n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        max_test_acc = checkpoint['max_test_acc']\n",
    "    \n",
    "    out_dir = os.path.join(args.out_dir, f'T{args.T}_b{args.b}_{args.opt}_lr{args.lr}')\n",
    "\n",
    "    if torch.cuda.is_available() and args.amp:\n",
    "        out_dir += '_amp'\n",
    "\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "        print(f'Mkdir {out_dir}.')\n",
    "\n",
    "    with open(os.path.join(out_dir, 'args.txt'), 'w', encoding='utf-8') as args_txt:\n",
    "        args_txt.write(str(args))\n",
    "\n",
    "    writer = SummaryWriter(out_dir, purge_step=start_epoch)\n",
    "    with open(os.path.join(out_dir, 'args.txt'), 'w', encoding='utf-8') as args_txt:\n",
    "        args_txt.write(str(args))\n",
    "        args_txt.write('\\n')\n",
    "        args_txt.write(' '.join(sys.argv))\n",
    "\n",
    "    encoder = encoding.PoissonEncoder() # 泊松分布决定是否释放脉冲，得到64*1*28*28\n",
    "\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        start_time = time.time()\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        train_samples = 0\n",
    "        for img, label in train_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            img = img.to(args.device)\n",
    "            label = label.to(args.device)\n",
    "            label_onehot = F.one_hot(label, 10).float() # 将label转换为10位one-hot编码\n",
    "\n",
    "            if scaler is not None: # 是否使用自动混合精度cuda\n",
    "                with amp.autocast():\n",
    "                    out_fr = 0.\n",
    "                    for t in range(args.T):\n",
    "                        encoded_img = encoder(img)\n",
    "                        out_fr += net(encoded_img)\n",
    "                    out_fr = out_fr / args.T\n",
    "                    loss = F.mse_loss(out_fr, label_onehot)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else: # 没有gpu加速，不使用\n",
    "                out_fr = 0.\n",
    "                for t in range(args.T):\n",
    "                    encoded_img = encoder(img) # 64*1*28*28\n",
    "                    res = net(encoded_img) # 64*10\n",
    "                    out_fr += res # 传入snn网络，得到所有时间步的累积输出\n",
    "\n",
    "                out_fr = out_fr / args.T # 对所有时间步取平均\n",
    "                loss = F.mse_loss(out_fr, label_onehot)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_samples += label.numel()\n",
    "            train_loss += loss.item() * label.numel()\n",
    "            train_acc += (out_fr.argmax(1) == label).float().sum().item() #取概率最大值的索引，与label比较，得到正确率\n",
    "\n",
    "            functional.reset_net(net)\n",
    "\n",
    "        train_time = time.time()\n",
    "        train_speed = train_samples / (train_time - start_time)\n",
    "        train_loss /= train_samples\n",
    "        train_acc /= train_samples\n",
    "\n",
    "        writer.add_scalar('train_loss', train_loss, epoch)\n",
    "        writer.add_scalar('train_acc', train_acc, epoch)\n",
    "\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        test_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for img, label in test_data_loader:\n",
    "                img = img.to(args.device)\n",
    "                label = label.to(args.device)\n",
    "                label_onehot = F.one_hot(label, 10).float()\n",
    "                out_fr = 0.\n",
    "                for t in range(args.T):\n",
    "                    encoded_img = encoder(img)\n",
    "                    out_fr += net(encoded_img)\n",
    "                out_fr = out_fr / args.T\n",
    "                loss = F.mse_loss(out_fr, label_onehot)\n",
    "\n",
    "                test_samples += label.numel()\n",
    "                test_loss += loss.item() * label.numel()\n",
    "                test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "                functional.reset_net(net)\n",
    "        test_time = time.time()\n",
    "        test_speed = test_samples / (test_time - train_time)\n",
    "        test_loss /= test_samples\n",
    "        test_acc /= test_samples\n",
    "        writer.add_scalar('test_loss', test_loss, epoch)\n",
    "        writer.add_scalar('test_acc', test_acc, epoch)\n",
    "\n",
    "        save_max = False\n",
    "        if test_acc > max_test_acc:\n",
    "            max_test_acc = test_acc\n",
    "            save_max = True\n",
    "\n",
    "        checkpoint = {\n",
    "            'net': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'max_test_acc': max_test_acc\n",
    "        }\n",
    "\n",
    "        if save_max:\n",
    "            torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_max.pth'))\n",
    "\n",
    "        torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_latest.pth'))\n",
    "\n",
    "        print(args)\n",
    "        print(out_dir)\n",
    "        print(f'epoch ={epoch}, train_loss ={train_loss: .4f}, train_acc ={train_acc: .4f}, test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}, max_test_acc ={max_test_acc: .4f}')\n",
    "        print(f'train speed ={train_speed: .4f} images/s, test speed ={test_speed: .4f} images/s')\n",
    "        print(f'escape time = {(datetime.datetime.now() + datetime.timedelta(seconds=(time.time() - start_time) * (args.epochs - epoch))).strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "\n",
    "    # 保存绘图用数据\n",
    "    net.eval()\n",
    "    # 注册钩子\n",
    "    output_layer = net.layer[-1] # 输出层\n",
    "    output_layer.v_seq = []\n",
    "    output_layer.s_seq = []\n",
    "    def save_hook(m, x, y):\n",
    "        m.v_seq.append(m.v.unsqueeze(0))\n",
    "        m.s_seq.append(y.unsqueeze(0))\n",
    "\n",
    "    output_layer.register_forward_hook(save_hook)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img, label = test_dataset[0]\n",
    "        img = img.to(args.device)\n",
    "        out_fr = 0.\n",
    "        for t in range(args.T):\n",
    "            encoded_img = encoder(img)\n",
    "            out_fr += net(encoded_img)\n",
    "        out_spikes_counter_frequency = (out_fr / args.T).cpu().numpy()\n",
    "        print(f'Firing rate: {out_spikes_counter_frequency}')\n",
    "\n",
    "        output_layer.v_seq = torch.cat(output_layer.v_seq)\n",
    "        output_layer.s_seq = torch.cat(output_layer.s_seq)\n",
    "        v_t_array = output_layer.v_seq.cpu().numpy().squeeze()  # v_t_array[i][j]表示神经元i在j时刻的电压值\n",
    "        np.save(\"v_t_array.npy\",v_t_array)\n",
    "        s_t_array = output_layer.s_seq.cpu().numpy().squeeze()  # s_t_array[i][j]表示神经元i在j时刻释放的脉冲，为0或1\n",
    "        np.save(\"s_t_array.npy\",s_t_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(T=100, device='cpu', b=64, epochs=100, j=4, data_dir=None, out_dir='./logs', resume=None, amp=False, opt='adam', momentum=0.9, lr=0.001, tau=2.0)\n",
      "SNN(\n",
      "  (layer): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1, step_mode=s)\n",
      "    (1): Linear(in_features=784, out_features=128, bias=False)\n",
      "    (2): LIFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=s, backend=torch, tau=2.0\n",
      "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "    )\n",
      "    (3): Linear(in_features=128, out_features=10, bias=False)\n",
      "    (4): LIFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=s, backend=torch, tau=2.0\n",
      "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Namespace(T=100, device='cpu', b=64, epochs=100, j=4, data_dir=None, out_dir='./logs', resume=None, amp=False, opt='adam', momentum=0.9, lr=0.001, tau=2.0)\n",
      "./logs\\T100_b64_adam_lr0.001\n",
      "epoch =0, train_loss = 0.0201, train_acc = 0.8884, test_loss = 0.0102, test_acc = 0.9458, max_test_acc = 0.9458\n",
      "train speed = 521.3505 images/s, test speed = 916.2064 images/s\n",
      "escape time = 2024-12-11 21:58:58\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 170\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    168\u001b[0m     out_fr \u001b[38;5;241m=\u001b[39m out_fr \u001b[38;5;241m/\u001b[39m args\u001b[38;5;241m.\u001b[39mT \u001b[38;5;66;03m# 对所有时间步取平均\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(out_fr, label_onehot)\n\u001b[1;32m--> 170\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    173\u001b[0m train_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mnumel()\n",
      "File \u001b[1;32mc:\\Users\\HYT\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HYT\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HYT\\anaconda3\\lib\\site-packages\\torch\\autograd\\function.py:274\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    271\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    272\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    273\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HYT\\anaconda3\\lib\\site-packages\\spikingjelly\\activation_based\\surrogate.py:678\u001b[0m, in \u001b[0;36matan.backward\u001b[1;34m(ctx, grad_output)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(ctx, grad_output):\n\u001b[1;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43matan_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaved_tensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
