{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "train1 = torch.load(r'E:\\snn_final\\ColoredMNIST\\train1.pt',weights_only=False)\n",
    "train2 = torch.load(r'E:\\snn_final\\ColoredMNIST\\train2.pt',weights_only=False)\n",
    "test = torch.load(r'E:\\snn_final\\ColoredMNIST\\test.pt',weights_only=False)\n",
    "\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "def dataset_load(raw_dataset):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for one_data in raw_dataset:\n",
    "        y.append(one_data[1])\n",
    "        image=one_data[0]\n",
    "        data=np.array(image)\n",
    "        data=data.transpose(2,0,1)\n",
    "        x.append(data)\n",
    "    x=torch.Tensor(np.array(x))\n",
    "    y=torch.Tensor(np.array(y))\n",
    "    ds=torch.utils.data.TensorDataset(x,y)\n",
    "    return ds\n",
    "\n",
    "train1_data = dataset_load(train1)\n",
    "train2_data = dataset_load(train2)\n",
    "test_data = dataset_load(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import numpy as np  \n",
    "from spikingjelly.activation_based import neuron, encoding, functional, surrogate, layer  \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "class ReversalLayer(torch.autograd.Function):  \n",
    "    @staticmethod  \n",
    "    def forward(ctx, x, alpha):  \n",
    "        ctx.alpha = alpha  \n",
    "        return x.view_as(x)  \n",
    "\n",
    "    @staticmethod  \n",
    "    def backward(ctx, grad_output):  \n",
    "        output = grad_output.neg() * ctx.alpha  \n",
    "        return output, None  \n",
    "\n",
    "class SpikingDANN(nn.Module):  \n",
    "    def __init__(self):  \n",
    "        super(SpikingDANN, self).__init__()  \n",
    "        self.feature_extractor = nn.Sequential(  \n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),  \n",
    "            nn.BatchNorm2d(6),  \n",
    "            nn.MaxPool2d(2, 2),  \n",
    "            nn.LeakyReLU(0.1),  \n",
    "            nn.Conv2d(6, 16, 5),  \n",
    "            nn.BatchNorm2d(16),  \n",
    "            nn.Dropout2d(),  \n",
    "            nn.MaxPool2d(2, 2),  \n",
    "            nn.LeakyReLU(0.1),  \n",
    "        )  \n",
    "        \n",
    "        # SNN Classifier with Leaky Integrate-and-Fire neurons  \n",
    "        self.classifier = nn.Sequential(  \n",
    "            nn.Linear(16 * 4 * 4, 32),  \n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan()),  \n",
    "            nn.Linear(32, 32),  \n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan()),  \n",
    "            nn.Linear(32, 2),  \n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan()),  \n",
    "        )  \n",
    "        \n",
    "        # Domain Discriminator  \n",
    "        self.domain_discriminator = nn.Sequential(  \n",
    "            nn.Linear(16 * 4 * 4, 16),  \n",
    "            nn.Dropout(0.3),\n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan()),  \n",
    "            nn.Linear(16, 2), \n",
    "            #neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan()),  \n",
    "        )  \n",
    "\n",
    "    def forward(self, input_spikes, alpha=1.0):  \n",
    "        # Extract features  \n",
    "        feature = self.feature_extractor(input_spikes)  \n",
    "        feature = feature.view(input_spikes.shape[0], -1)\n",
    "\n",
    "        class_output = self.classifier(feature)  \n",
    "        domain_output = self.domain_discriminator(ReversalLayer.apply(feature, alpha))  \n",
    "        \n",
    "        return class_output, domain_output  \n",
    "    \n",
    "def classification_accuracy(data_iter, net, domain_flag, encoder):  \n",
    "    class_acc_sum, domain_acc_sum, n = 0.0, 0.0, 0  \n",
    "    with torch.no_grad():  \n",
    "        for X, y in data_iter:  \n",
    "            net.eval()  \n",
    "            spike_input = encoder(X)  # Encode to spikes\n",
    "\n",
    "            class_output, domain_output = net(spike_input)  \n",
    "            class_acc_sum += (class_output.argmax(dim=1) == y).float().sum().item()  \n",
    "            domain_acc_sum += (domain_output.argmax(dim=1) == domain_flag).float().sum().item()  \n",
    "            n += y.shape[0]  \n",
    "    return class_acc_sum / n, domain_acc_sum / n  \n",
    "  \n",
    "def evaluate_accuracy(data_iter, net, encoder, T):  \n",
    "    acc_sum, n = 0.0, 0\n",
    "    net.eval()  \n",
    "    with torch.no_grad():  \n",
    "        for X, y in data_iter:   \n",
    "            for t in range(T):\n",
    "                spike_input = encoder(X)  # Encode to spikes  \n",
    "                class_output, _ = net(spike_input)  \n",
    "                if t == 0:\n",
    "                    out_fr = torch.zeros_like(class_output) \n",
    "                out_fr += class_output\n",
    "            out_fr = out_fr / T\n",
    "            acc_sum += (out_fr.argmax(1) == y).float().sum().item()  \n",
    "            n += y.numel()\n",
    "            functional.reset_net(net)\n",
    "    return acc_sum / n \n",
    "    \n",
    "\n",
    "def train_snn(net, train1_data, train2_data, test_data):  \n",
    "    class parser:  \n",
    "        def __init__(self):  \n",
    "            self.T = 5  \n",
    "            self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'  \n",
    "            self.epochs = 50  \n",
    "            self.b = 128  \n",
    "            self.j = 4  \n",
    "            self.out_dir = './logs'  \n",
    "            self.resume = None\n",
    "            self.amp = True  \n",
    "            self.opt = 'adam'  \n",
    "            self.lr = 1e-3  \n",
    "            self.tau = 2.0  \n",
    "            self.alpha_domain = 1.0\n",
    "    \n",
    "    args = parser()\n",
    "    \n",
    "    encoder = encoding.PoissonEncoder()\n",
    "    loss_class_func = nn.CrossEntropyLoss()\n",
    "    loss_domain_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=args.lr)\n",
    "\n",
    "    # 训练日志目录  \n",
    "    out_dir = os.path.join(  \n",
    "        args.out_dir,   \n",
    "        f'DANN_SNN_T{args.T}'  \n",
    "    )  \n",
    "    os.makedirs(out_dir, exist_ok=True)  \n",
    "\n",
    "    writer = SummaryWriter(log_dir=out_dir)\n",
    "    \n",
    "    for epoch in range(args.epochs):  \n",
    "        net.train()\n",
    "        train1_acc, train2_acc, class_loss, domain1_loss, domain2_loss = 0,0,0,0,0 \n",
    "        domain1_acc, domain2_acc = 0,0\n",
    "        train_samples = 0\n",
    "        p = float(epoch)/float(args.epochs)\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1  \n",
    "        for (source_img, source_class), (target_img, target_class) in zip(train1_data, train2_data):  \n",
    "            optimizer.zero_grad()  \n",
    "            source_spikes = encoder(source_img) # 128,3,28,28\n",
    "            target_spikes = encoder(target_img)   \n",
    "            source_domain = torch.zeros(source_img.shape[0],dtype=torch.long).to(source_img.device)  \n",
    "            target_domain = torch.ones(target_img.shape[0],dtype=torch.long).to(target_img.device)  \n",
    "            source_class = source_class.long()\n",
    "            target_class = target_class.long()\n",
    " \n",
    "            for t in range(args.T):\n",
    "                class_output_source, domain_output_source = net(source_spikes,alpha=alpha)  \n",
    "                class_output_target, domain_output_target = net(target_spikes,alpha=alpha)  \n",
    "                \n",
    "                if t == 0:\n",
    "                    out_fr_class_s = torch.zeros_like(class_output_source)  \n",
    "                    out_fr_domain_s = torch.zeros_like(domain_output_source)  \n",
    "                    out_fr_class_t = torch.zeros_like(class_output_target)  \n",
    "                    out_fr_domain_t = torch.zeros_like(domain_output_target) \n",
    "                \n",
    "                out_fr_class_s += class_output_source\n",
    "                out_fr_domain_s += domain_output_source\n",
    "                out_fr_class_t += class_output_target\n",
    "                out_fr_domain_t += domain_output_target\n",
    "            out_fr_class_s = out_fr_class_s / args.T\n",
    "            out_fr_domain_s = out_fr_domain_s / args.T\n",
    "            out_fr_class_t = out_fr_class_t / args.T\n",
    "            out_fr_domain_t = out_fr_domain_t / args.T\n",
    "\n",
    "            # Calculate losses  \n",
    "            loss_class = loss_class_func(out_fr_class_s, source_class)  \n",
    "            loss_domain_source = loss_domain_func(out_fr_domain_s, source_domain)  \n",
    "            loss_domain_target = loss_domain_func(out_fr_domain_t, target_domain)   \n",
    "            loss = loss_class + loss_domain_source + loss_domain_target  \n",
    "            \n",
    "            loss.backward()  # Backpropagation  \n",
    "            optimizer.step()  # Optimization  \n",
    "\n",
    "            train_samples += source_class.size(0)\n",
    "            class_loss += loss_class.item() * source_class.size(0)\n",
    "            domain1_loss += loss_domain_source.item() * source_class.size(0)\n",
    "            domain2_loss += loss_domain_target.item() * source_class.size(0)\n",
    "            train1_acc += (out_fr_class_s.argmax(1) == source_class).float().sum().item()\n",
    "            train2_acc += (out_fr_class_t.argmax(1) == target_class).float().sum().item()\n",
    "            domain1_acc += (out_fr_domain_s.argmax(1) == source_domain).float().sum().item()\n",
    "            domain2_acc += (out_fr_domain_t.argmax(1) == target_domain).float().sum().item()\n",
    "\n",
    "            functional.reset_net(net)\n",
    "\n",
    "        class_loss /= train_samples\n",
    "        domain1_loss /= train_samples\n",
    "        domain2_loss /= train_samples\n",
    "        train1_acc /= train_samples\n",
    "        train2_acc /= train_samples\n",
    "        domain1_acc /= train_samples\n",
    "        domain2_acc /= train_samples\n",
    "\n",
    "        acc = evaluate_accuracy(test_data, net, encoder, args.T)\n",
    "\n",
    "        writer.add_scalar('Class Acc on source', train1_acc, epoch)\n",
    "        writer.add_scalar('Class Acc on target', train2_acc, epoch)\n",
    "        writer.add_scalar('Domain Acc on source', domain1_acc, epoch)\n",
    "        writer.add_scalar('Domain Acc on target', domain2_acc, epoch)\n",
    "        writer.add_scalar('Class Loss', class_loss, epoch)\n",
    "        writer.add_scalar('Domain Loss on source', domain1_loss, epoch)\n",
    "        writer.add_scalar('Domain Loss on target', domain2_loss, epoch)\n",
    "        writer.add_scalar('Class Acc on test', acc, epoch)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}, Class Loss: {class_loss:.4f}, Domain Source Loss: {domain1_loss:.4f}, Domain Target Loss: {domain2_loss:.4f}')   \n",
    "        print(f'Class Acc on train_source: {train1_acc:.4f}, Domain Acc on train_source: {domain1_acc:.4f}')  \n",
    "        print(f'Class Acc on train_target: {train2_acc:.4f}, Domain Acc on train_target: {domain2_acc:.4f}')  \n",
    "        print(f'Class Acc on test: {acc:.4f}')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Class Loss: 0.6347, Domain Source Loss: 0.7099, Domain Target Loss: 0.6803\n",
      "Class Acc on train_source: 0.6492, Domain Acc on train_source: 0.4229\n",
      "Class Acc on train_target: 0.7405, Domain Acc on train_target: 0.5836\n",
      "Class Acc on test: 0.0998\n",
      "Epoch 2, Class Loss: 0.5151, Domain Source Loss: 0.6931, Domain Target Loss: 0.6845\n",
      "Class Acc on train_source: 0.7976, Domain Acc on train_source: 0.5286\n",
      "Class Acc on train_target: 0.8970, Domain Acc on train_target: 0.5504\n",
      "Class Acc on test: 0.1668\n",
      "Epoch 3, Class Loss: 0.5142, Domain Source Loss: 0.6891, Domain Target Loss: 0.6829\n",
      "Class Acc on train_source: 0.7984, Domain Acc on train_source: 0.5400\n",
      "Class Acc on train_target: 0.8969, Domain Acc on train_target: 0.5551\n",
      "Class Acc on test: 0.1558\n",
      "Epoch 4, Class Loss: 0.5145, Domain Source Loss: 0.6902, Domain Target Loss: 0.6848\n",
      "Class Acc on train_source: 0.7973, Domain Acc on train_source: 0.5356\n",
      "Class Acc on train_target: 0.8960, Domain Acc on train_target: 0.5541\n",
      "Class Acc on test: 0.2544\n",
      "Epoch 5, Class Loss: 0.5137, Domain Source Loss: 0.6877, Domain Target Loss: 0.6837\n",
      "Class Acc on train_source: 0.7957, Domain Acc on train_source: 0.5437\n",
      "Class Acc on train_target: 0.8915, Domain Acc on train_target: 0.5606\n",
      "Class Acc on test: 0.2422\n",
      "Epoch 6, Class Loss: 0.5125, Domain Source Loss: 0.6868, Domain Target Loss: 0.6824\n",
      "Class Acc on train_source: 0.7952, Domain Acc on train_source: 0.5483\n",
      "Class Acc on train_target: 0.8905, Domain Acc on train_target: 0.5618\n",
      "Class Acc on test: 0.3095\n",
      "Epoch 7, Class Loss: 0.5117, Domain Source Loss: 0.6858, Domain Target Loss: 0.6841\n",
      "Class Acc on train_source: 0.7953, Domain Acc on train_source: 0.5454\n",
      "Class Acc on train_target: 0.8874, Domain Acc on train_target: 0.5623\n",
      "Class Acc on test: 0.2448\n",
      "Epoch 8, Class Loss: 0.5116, Domain Source Loss: 0.6854, Domain Target Loss: 0.6843\n",
      "Class Acc on train_source: 0.7953, Domain Acc on train_source: 0.5482\n",
      "Class Acc on train_target: 0.8908, Domain Acc on train_target: 0.5527\n",
      "Class Acc on test: 0.2394\n",
      "Epoch 9, Class Loss: 0.5113, Domain Source Loss: 0.6854, Domain Target Loss: 0.6797\n",
      "Class Acc on train_source: 0.7967, Domain Acc on train_source: 0.5575\n",
      "Class Acc on train_target: 0.8899, Domain Acc on train_target: 0.5643\n",
      "Class Acc on test: 0.3228\n",
      "Epoch 10, Class Loss: 0.5133, Domain Source Loss: 0.6844, Domain Target Loss: 0.6812\n",
      "Class Acc on train_source: 0.7956, Domain Acc on train_source: 0.5565\n",
      "Class Acc on train_target: 0.8894, Domain Acc on train_target: 0.5601\n",
      "Class Acc on test: 0.1757\n",
      "Epoch 11, Class Loss: 0.5153, Domain Source Loss: 0.6826, Domain Target Loss: 0.6814\n",
      "Class Acc on train_source: 0.7960, Domain Acc on train_source: 0.5674\n",
      "Class Acc on train_target: 0.8873, Domain Acc on train_target: 0.5620\n",
      "Class Acc on test: 0.2256\n",
      "Epoch 12, Class Loss: 0.5124, Domain Source Loss: 0.6836, Domain Target Loss: 0.6700\n",
      "Class Acc on train_source: 0.7939, Domain Acc on train_source: 0.5773\n",
      "Class Acc on train_target: 0.8862, Domain Acc on train_target: 0.5748\n",
      "Class Acc on test: 0.3417\n",
      "Epoch 13, Class Loss: 0.5103, Domain Source Loss: 0.6789, Domain Target Loss: 0.6716\n",
      "Class Acc on train_source: 0.7927, Domain Acc on train_source: 0.5864\n",
      "Class Acc on train_target: 0.8828, Domain Acc on train_target: 0.5709\n",
      "Class Acc on test: 0.2958\n",
      "Epoch 14, Class Loss: 0.5106, Domain Source Loss: 0.6706, Domain Target Loss: 0.6648\n",
      "Class Acc on train_source: 0.7957, Domain Acc on train_source: 0.5945\n",
      "Class Acc on train_target: 0.8890, Domain Acc on train_target: 0.5877\n",
      "Class Acc on test: 0.2619\n",
      "Epoch 15, Class Loss: 0.5091, Domain Source Loss: 0.6747, Domain Target Loss: 0.6618\n",
      "Class Acc on train_source: 0.7946, Domain Acc on train_source: 0.5941\n",
      "Class Acc on train_target: 0.8850, Domain Acc on train_target: 0.5857\n",
      "Class Acc on test: 0.2514\n",
      "Epoch 16, Class Loss: 0.5096, Domain Source Loss: 0.6707, Domain Target Loss: 0.6621\n",
      "Class Acc on train_source: 0.7946, Domain Acc on train_source: 0.5987\n",
      "Class Acc on train_target: 0.8859, Domain Acc on train_target: 0.5884\n",
      "Class Acc on test: 0.2831\n",
      "Epoch 17, Class Loss: 0.5103, Domain Source Loss: 0.6626, Domain Target Loss: 0.6516\n",
      "Class Acc on train_source: 0.7960, Domain Acc on train_source: 0.6136\n",
      "Class Acc on train_target: 0.8903, Domain Acc on train_target: 0.6107\n",
      "Class Acc on test: 0.2978\n",
      "Epoch 18, Class Loss: 0.5103, Domain Source Loss: 0.6613, Domain Target Loss: 0.6495\n",
      "Class Acc on train_source: 0.7923, Domain Acc on train_source: 0.6184\n",
      "Class Acc on train_target: 0.8808, Domain Acc on train_target: 0.6092\n",
      "Class Acc on test: 0.2606\n",
      "Epoch 19, Class Loss: 0.5087, Domain Source Loss: 0.6555, Domain Target Loss: 0.6456\n",
      "Class Acc on train_source: 0.7940, Domain Acc on train_source: 0.6261\n",
      "Class Acc on train_target: 0.8804, Domain Acc on train_target: 0.6109\n",
      "Class Acc on test: 0.2204\n",
      "Epoch 20, Class Loss: 0.5080, Domain Source Loss: 0.6568, Domain Target Loss: 0.6458\n",
      "Class Acc on train_source: 0.7957, Domain Acc on train_source: 0.6224\n",
      "Class Acc on train_target: 0.8853, Domain Acc on train_target: 0.6099\n",
      "Class Acc on test: 0.2769\n",
      "Epoch 21, Class Loss: 0.5078, Domain Source Loss: 0.6504, Domain Target Loss: 0.6399\n",
      "Class Acc on train_source: 0.7951, Domain Acc on train_source: 0.6329\n",
      "Class Acc on train_target: 0.8821, Domain Acc on train_target: 0.6226\n",
      "Class Acc on test: 0.2648\n",
      "Epoch 22, Class Loss: 0.5079, Domain Source Loss: 0.6427, Domain Target Loss: 0.6358\n",
      "Class Acc on train_source: 0.7941, Domain Acc on train_source: 0.6404\n",
      "Class Acc on train_target: 0.8858, Domain Acc on train_target: 0.6239\n",
      "Class Acc on test: 0.2357\n",
      "Epoch 23, Class Loss: 0.5086, Domain Source Loss: 0.6399, Domain Target Loss: 0.6330\n",
      "Class Acc on train_source: 0.7936, Domain Acc on train_source: 0.6439\n",
      "Class Acc on train_target: 0.8838, Domain Acc on train_target: 0.6314\n",
      "Class Acc on test: 0.2940\n",
      "Epoch 24, Class Loss: 0.5097, Domain Source Loss: 0.6435, Domain Target Loss: 0.6330\n",
      "Class Acc on train_source: 0.7915, Domain Acc on train_source: 0.6479\n",
      "Class Acc on train_target: 0.8759, Domain Acc on train_target: 0.6260\n",
      "Class Acc on test: 0.3207\n",
      "Epoch 25, Class Loss: 0.5098, Domain Source Loss: 0.6392, Domain Target Loss: 0.6273\n",
      "Class Acc on train_source: 0.7919, Domain Acc on train_source: 0.6511\n",
      "Class Acc on train_target: 0.8769, Domain Acc on train_target: 0.6326\n",
      "Class Acc on test: 0.3084\n",
      "Epoch 26, Class Loss: 0.5080, Domain Source Loss: 0.6401, Domain Target Loss: 0.6281\n",
      "Class Acc on train_source: 0.7944, Domain Acc on train_source: 0.6467\n",
      "Class Acc on train_target: 0.8820, Domain Acc on train_target: 0.6353\n",
      "Class Acc on test: 0.2728\n",
      "Epoch 27, Class Loss: 0.5086, Domain Source Loss: 0.6349, Domain Target Loss: 0.6230\n",
      "Class Acc on train_source: 0.7942, Domain Acc on train_source: 0.6485\n",
      "Class Acc on train_target: 0.8767, Domain Acc on train_target: 0.6364\n",
      "Class Acc on test: 0.2490\n",
      "Epoch 28, Class Loss: 0.5085, Domain Source Loss: 0.6361, Domain Target Loss: 0.6332\n",
      "Class Acc on train_source: 0.7950, Domain Acc on train_source: 0.6520\n",
      "Class Acc on train_target: 0.8854, Domain Acc on train_target: 0.6281\n",
      "Class Acc on test: 0.2979\n",
      "Epoch 29, Class Loss: 0.5075, Domain Source Loss: 0.6334, Domain Target Loss: 0.6304\n",
      "Class Acc on train_source: 0.7956, Domain Acc on train_source: 0.6511\n",
      "Class Acc on train_target: 0.8887, Domain Acc on train_target: 0.6315\n",
      "Class Acc on test: 0.1651\n",
      "Epoch 30, Class Loss: 0.5078, Domain Source Loss: 0.6337, Domain Target Loss: 0.6273\n",
      "Class Acc on train_source: 0.7968, Domain Acc on train_source: 0.6548\n",
      "Class Acc on train_target: 0.8828, Domain Acc on train_target: 0.6378\n",
      "Class Acc on test: 0.1670\n",
      "Epoch 31, Class Loss: 0.5084, Domain Source Loss: 0.6317, Domain Target Loss: 0.6254\n",
      "Class Acc on train_source: 0.7922, Domain Acc on train_source: 0.6535\n",
      "Class Acc on train_target: 0.8788, Domain Acc on train_target: 0.6380\n",
      "Class Acc on test: 0.3125\n",
      "Epoch 32, Class Loss: 0.5084, Domain Source Loss: 0.6356, Domain Target Loss: 0.6380\n",
      "Class Acc on train_source: 0.7942, Domain Acc on train_source: 0.6504\n",
      "Class Acc on train_target: 0.8800, Domain Acc on train_target: 0.6214\n",
      "Class Acc on test: 0.2537\n",
      "Epoch 33, Class Loss: 0.5082, Domain Source Loss: 0.6358, Domain Target Loss: 0.6358\n",
      "Class Acc on train_source: 0.7946, Domain Acc on train_source: 0.6494\n",
      "Class Acc on train_target: 0.8831, Domain Acc on train_target: 0.6256\n",
      "Class Acc on test: 0.2410\n",
      "Epoch 34, Class Loss: 0.5085, Domain Source Loss: 0.6296, Domain Target Loss: 0.6313\n",
      "Class Acc on train_source: 0.7948, Domain Acc on train_source: 0.6519\n",
      "Class Acc on train_target: 0.8875, Domain Acc on train_target: 0.6314\n",
      "Class Acc on test: 0.1863\n",
      "Epoch 35, Class Loss: 0.5076, Domain Source Loss: 0.6389, Domain Target Loss: 0.6309\n",
      "Class Acc on train_source: 0.7958, Domain Acc on train_source: 0.6421\n",
      "Class Acc on train_target: 0.8846, Domain Acc on train_target: 0.6288\n",
      "Class Acc on test: 0.2928\n",
      "Epoch 36, Class Loss: 0.5080, Domain Source Loss: 0.6361, Domain Target Loss: 0.6345\n",
      "Class Acc on train_source: 0.7939, Domain Acc on train_source: 0.6475\n",
      "Class Acc on train_target: 0.8806, Domain Acc on train_target: 0.6291\n",
      "Class Acc on test: 0.2506\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(test_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Start training  \u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrain_snn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain1_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain2_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m  \n",
      "Cell \u001b[1;32mIn[72], line 170\u001b[0m, in \u001b[0;36mtrain_snn\u001b[1;34m(net, train1_data, train2_data, test_data)\u001b[0m\n\u001b[0;32m    167\u001b[0m loss_domain_target \u001b[38;5;241m=\u001b[39m loss_domain_func(out_fr_domain_t, target_domain)   \n\u001b[0;32m    168\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_class \u001b[38;5;241m+\u001b[39m loss_domain_source \u001b[38;5;241m+\u001b[39m loss_domain_target  \n\u001b[1;32m--> 170\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backpropagation  \u001b[39;00m\n\u001b[0;32m    171\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Optimization  \u001b[39;00m\n\u001b[0;32m    173\u001b[0m train_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m source_class\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cyd\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cyd\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cyd\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32mc:\\Users\\cyd\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m     )\n\u001b[0;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cyd\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spikingjelly\\activation_based\\surrogate.py:678\u001b[0m, in \u001b[0;36matan.backward\u001b[1;34m(ctx, grad_output)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(ctx, grad_output):\n\u001b[1;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43matan_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaved_tensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = SpikingDANN()  # Move to GPU if available  \n",
    "  \n",
    "\n",
    "train1_loader = torch.utils.data.DataLoader(train1_data, batch_size=128, shuffle=True)\n",
    "train2_loader = torch.utils.data.DataLoader(train2_data, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=True)\n",
    "\n",
    "# Start training  \n",
    "train_snn(net, train1_loader, train2_loader, test_loader)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
