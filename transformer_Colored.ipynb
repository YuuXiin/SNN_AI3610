{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "train1 = torch.load(r'E:\\snn_final\\ColoredMNIST\\train1.pt',weights_only=False)\n",
    "train2 = torch.load(r'E:\\snn_final\\ColoredMNIST\\train2.pt',weights_only=False)\n",
    "test = torch.load(r'E:\\snn_final\\ColoredMNIST\\test.pt',weights_only=False)\n",
    "\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "def dataset_load(raw_dataset):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for one_data in raw_dataset:\n",
    "        y.append(one_data[1])\n",
    "        image=one_data[0]\n",
    "        data=np.array(image)\n",
    "        data=data.transpose(2,0,1)\n",
    "        x.append(data)\n",
    "    x=torch.Tensor(np.array(x))\n",
    "    y=torch.Tensor(np.array(y))\n",
    "    ds=torch.utils.data.TensorDataset(x,y)\n",
    "    return ds\n",
    "\n",
    "train1_data = dataset_load(train1)\n",
    "train2_data = dataset_load(train2)\n",
    "test_data = dataset_load(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import numpy as np  \n",
    "from spikingjelly.activation_based import neuron, encoding, functional, surrogate, layer  \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "class ReversalLayer(torch.autograd.Function):  \n",
    "    @staticmethod  \n",
    "    def forward(ctx, x, alpha):  \n",
    "        ctx.alpha = alpha  \n",
    "        return x.view_as(x)  \n",
    "\n",
    "    @staticmethod  \n",
    "    def backward(ctx, grad_output):  \n",
    "        output = grad_output.neg() * ctx.alpha  \n",
    "        return output, None  \n",
    "\n",
    "class SpikingDANN(nn.Module):  \n",
    "    def __init__(self, img_size=28, patch_size=7, in_channels=3, embed_dim=128, num_heads=4, num_classes=2, tau=2.0):  \n",
    "        super(SpikingDANN, self).__init__()  \n",
    "         \n",
    "        self.patch_embed = nn.Sequential(  \n",
    "            nn.Conv2d(in_channels, embed_dim,   \n",
    "                      kernel_size=patch_size,   \n",
    "                      stride=patch_size),  \n",
    "            layer.BatchNorm2d(embed_dim)  \n",
    "        )  \n",
    "        \n",
    "        # 位置编码  \n",
    "        num_patches = (img_size // patch_size) ** 2  \n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))  \n",
    "        \n",
    "        # Transformer 编码器  \n",
    "        self.transformer_encoder = nn.TransformerEncoder(  \n",
    "            nn.TransformerEncoderLayer(  \n",
    "                d_model=embed_dim,   \n",
    "                nhead=num_heads,  \n",
    "                dim_feedforward=embed_dim*4,  \n",
    "                activation=F.relu  \n",
    "            ),  \n",
    "            num_layers=2  \n",
    "        )  \n",
    "        \n",
    "        # SNN Classifier with Leaky Integrate-and-Fire neurons  \n",
    "        self.classifier = nn.Sequential(  \n",
    "            nn.Linear(16 * 128, 32),  \n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan()),  \n",
    "            nn.Linear(32, 32),  \n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan()),  \n",
    "            nn.Linear(32, num_classes),  \n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan()),  \n",
    "        )  \n",
    "        \n",
    "        # Domain Discriminator  \n",
    "        self.domain_discriminator = nn.Sequential(  \n",
    "            nn.Linear(16 * 128, 16),  \n",
    "            nn.Dropout(0.3),\n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan()),  \n",
    "            nn.Linear(16, 2), \n",
    "        )  \n",
    "\n",
    "    def forward(self, input_spikes, alpha=1.0):  \n",
    "        x = self.patch_embed(input_spikes)\n",
    "        B, C, H, W = x.shape  \n",
    "        x = x.view(B, C, H*W).transpose(1, 2)   \n",
    "        x += self.pos_embed  \n",
    "        x = self.transformer_encoder(x)  \n",
    "        feature = x.view(B, -1)  \n",
    "\n",
    "        class_output = self.classifier(feature)  \n",
    "        domain_output = self.domain_discriminator(ReversalLayer.apply(feature, alpha))  \n",
    "        \n",
    "        return class_output, domain_output  \n",
    "      \n",
    "def evaluate_accuracy(data_iter, net, encoder, T):  \n",
    "    acc_sum, n = 0.0, 0\n",
    "    net.eval()  \n",
    "    with torch.no_grad():  \n",
    "        for X, y in data_iter:   \n",
    "            for t in range(T):\n",
    "                spike_input = encoder(X)  # Encode to spikes  \n",
    "                class_output, _ = net(spike_input)  \n",
    "                if t == 0:\n",
    "                    out_fr = torch.zeros_like(class_output) \n",
    "                out_fr += class_output\n",
    "            out_fr = out_fr / T\n",
    "            acc_sum += (out_fr.argmax(1) == y).float().sum().item()  \n",
    "            n += y.numel()\n",
    "            functional.reset_net(net)\n",
    "    return acc_sum / n \n",
    "    \n",
    "\n",
    "def train_snn(net, train1_data, train2_data, test_data):  \n",
    "    class parser:  \n",
    "        def __init__(self):  \n",
    "            self.T = 1  \n",
    "            self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'  \n",
    "            self.epochs = 10  \n",
    "            self.b = 128  \n",
    "            self.j = 4  \n",
    "            self.out_dir = './logs'  \n",
    "            self.resume = None\n",
    "            self.amp = True  \n",
    "            self.opt = 'adam'  \n",
    "            self.lr = 1e-3  \n",
    "            self.tau = 2.0  \n",
    "            self.alpha_domain = 1.0\n",
    "    \n",
    "    args = parser()\n",
    "    \n",
    "    encoder = encoding.PoissonEncoder()\n",
    "    loss_class_func = nn.CrossEntropyLoss()\n",
    "    loss_domain_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=args.lr)\n",
    "\n",
    "    # 训练日志目录  \n",
    "    out_dir = os.path.join(  \n",
    "        args.out_dir,   \n",
    "        f'Mamba_DANN_SNN_T{args.T}'  \n",
    "    )  \n",
    "    os.makedirs(out_dir, exist_ok=True)  \n",
    "\n",
    "    writer = SummaryWriter(log_dir=out_dir)\n",
    "    \n",
    "    for epoch in range(args.epochs):  \n",
    "        net.train()\n",
    "        train1_acc, train2_acc, class_loss, domain1_loss, domain2_loss = 0,0,0,0,0 \n",
    "        domain1_acc, domain2_acc = 0,0\n",
    "        train_samples = 0\n",
    "        p = float(epoch)/float(args.epochs)\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1  \n",
    "        for (source_img, source_class), (target_img, target_class) in zip(train1_data, train2_data):  \n",
    "            optimizer.zero_grad()  \n",
    "            source_spikes = encoder(source_img) # 128,3,28,28\n",
    "            target_spikes = encoder(target_img)   \n",
    "            source_domain = torch.zeros(source_img.shape[0],dtype=torch.long).to(source_img.device)  \n",
    "            target_domain = torch.ones(target_img.shape[0],dtype=torch.long).to(target_img.device)  \n",
    "            source_class = source_class.long()\n",
    "            target_class = target_class.long()\n",
    " \n",
    "            for t in range(args.T):\n",
    "                class_output_source, domain_output_source = net(source_spikes,alpha=alpha)  \n",
    "                class_output_target, domain_output_target = net(target_spikes,alpha=alpha)  \n",
    "                \n",
    "                if t == 0:\n",
    "                    out_fr_class_s = torch.zeros_like(class_output_source)  \n",
    "                    out_fr_domain_s = torch.zeros_like(domain_output_source)  \n",
    "                    out_fr_class_t = torch.zeros_like(class_output_target)  \n",
    "                    out_fr_domain_t = torch.zeros_like(domain_output_target) \n",
    "                \n",
    "                out_fr_class_s += class_output_source\n",
    "                out_fr_domain_s += domain_output_source\n",
    "                out_fr_class_t += class_output_target\n",
    "                out_fr_domain_t += domain_output_target\n",
    "            out_fr_class_s = out_fr_class_s / args.T\n",
    "            out_fr_domain_s = out_fr_domain_s / args.T\n",
    "            out_fr_class_t = out_fr_class_t / args.T\n",
    "            out_fr_domain_t = out_fr_domain_t / args.T\n",
    "\n",
    "            # Calculate losses  \n",
    "            loss_class = loss_class_func(out_fr_class_s, source_class)  \n",
    "            loss_domain_source = loss_domain_func(out_fr_domain_s, source_domain)  \n",
    "            loss_domain_target = loss_domain_func(out_fr_domain_t, target_domain)   \n",
    "            loss = loss_class + loss_domain_source + loss_domain_target  \n",
    "            \n",
    "            loss.backward()  # Backpropagation  \n",
    "            optimizer.step()  # Optimization  \n",
    "\n",
    "            train_samples += source_class.size(0)\n",
    "            class_loss += loss_class.item() * source_class.size(0)\n",
    "            domain1_loss += loss_domain_source.item() * source_class.size(0)\n",
    "            domain2_loss += loss_domain_target.item() * source_class.size(0)\n",
    "            train1_acc += (out_fr_class_s.argmax(1) == source_class).float().sum().item()\n",
    "            train2_acc += (out_fr_class_t.argmax(1) == target_class).float().sum().item()\n",
    "            domain1_acc += (out_fr_domain_s.argmax(1) == source_domain).float().sum().item()\n",
    "            domain2_acc += (out_fr_domain_t.argmax(1) == target_domain).float().sum().item()\n",
    "\n",
    "            functional.reset_net(net)\n",
    "\n",
    "        class_loss /= train_samples\n",
    "        domain1_loss /= train_samples\n",
    "        domain2_loss /= train_samples\n",
    "        train1_acc /= train_samples\n",
    "        train2_acc /= train_samples\n",
    "        domain1_acc /= train_samples\n",
    "        domain2_acc /= train_samples\n",
    "\n",
    "        acc = evaluate_accuracy(test_data, net, encoder, args.T)\n",
    "\n",
    "        writer.add_scalar('Class Acc on source', train1_acc, epoch)\n",
    "        writer.add_scalar('Class Acc on target', train2_acc, epoch)\n",
    "        writer.add_scalar('Domain Acc on source', domain1_acc, epoch)\n",
    "        writer.add_scalar('Domain Acc on target', domain2_acc, epoch)\n",
    "        writer.add_scalar('Class Loss', class_loss, epoch)\n",
    "        writer.add_scalar('Domain Loss on source', domain1_loss, epoch)\n",
    "        writer.add_scalar('Domain Loss on target', domain2_loss, epoch)\n",
    "        writer.add_scalar('Class Acc on test', acc, epoch)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}, Class Loss: {class_loss:.4f}, Domain Source Loss: {domain1_loss:.4f}, Domain Target Loss: {domain2_loss:.4f}')   \n",
    "        print(f'Class Acc on train_source: {train1_acc:.4f}, Domain Acc on train_source: {domain1_acc:.4f}')  \n",
    "        print(f'Class Acc on train_target: {train2_acc:.4f}, Domain Acc on train_target: {domain2_acc:.4f}')  \n",
    "        print(f'Class Acc on test: {acc:.4f}')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Class Loss: 0.6418, Domain Source Loss: 0.7087, Domain Target Loss: 0.5851\n",
      "Class Acc on train_source: 0.6037, Domain Acc on train_source: 0.4436\n",
      "Class Acc on train_target: 0.6061, Domain Acc on train_target: 0.7821\n",
      "Class Acc on test: 0.0997\n",
      "Epoch 2, Class Loss: 0.5195, Domain Source Loss: 0.7084, Domain Target Loss: 0.6946\n",
      "Class Acc on train_source: 0.7920, Domain Acc on train_source: 0.5219\n",
      "Class Acc on train_target: 0.7103, Domain Acc on train_target: 0.5343\n",
      "Class Acc on test: 0.1002\n",
      "Epoch 3, Class Loss: 0.5163, Domain Source Loss: 0.6889, Domain Target Loss: 0.7016\n",
      "Class Acc on train_source: 0.7944, Domain Acc on train_source: 0.5403\n",
      "Class Acc on train_target: 0.7358, Domain Acc on train_target: 0.4933\n",
      "Class Acc on test: 0.0997\n",
      "Epoch 4, Class Loss: 0.5599, Domain Source Loss: 0.6929, Domain Target Loss: 0.6835\n",
      "Class Acc on train_source: 0.7589, Domain Acc on train_source: 0.5010\n",
      "Class Acc on train_target: 0.7749, Domain Acc on train_target: 0.5900\n",
      "Class Acc on test: 0.0997\n",
      "Epoch 5, Class Loss: 0.5173, Domain Source Loss: 0.6877, Domain Target Loss: 0.7009\n",
      "Class Acc on train_source: 0.7955, Domain Acc on train_source: 0.5435\n",
      "Class Acc on train_target: 0.8240, Domain Acc on train_target: 0.4852\n",
      "Class Acc on test: 0.1019\n",
      "Epoch 6, Class Loss: 0.5149, Domain Source Loss: 0.7004, Domain Target Loss: 0.6877\n",
      "Class Acc on train_source: 0.7984, Domain Acc on train_source: 0.4551\n",
      "Class Acc on train_target: 0.8959, Domain Acc on train_target: 0.5487\n",
      "Class Acc on test: 0.0999\n",
      "Epoch 7, Class Loss: 0.5150, Domain Source Loss: 0.6943, Domain Target Loss: 0.6915\n",
      "Class Acc on train_source: 0.7984, Domain Acc on train_source: 0.4370\n",
      "Class Acc on train_target: 0.8956, Domain Acc on train_target: 0.5801\n",
      "Class Acc on test: 0.0997\n",
      "Epoch 8, Class Loss: 0.5153, Domain Source Loss: 0.6901, Domain Target Loss: 0.6901\n",
      "Class Acc on train_source: 0.7980, Domain Acc on train_source: 0.4879\n",
      "Class Acc on train_target: 0.8909, Domain Acc on train_target: 0.6091\n",
      "Class Acc on test: 0.1000\n",
      "Epoch 9, Class Loss: 0.5203, Domain Source Loss: 0.6884, Domain Target Loss: 0.6934\n",
      "Class Acc on train_source: 0.7936, Domain Acc on train_source: 0.4902\n",
      "Class Acc on train_target: 0.8414, Domain Acc on train_target: 0.5739\n",
      "Class Acc on test: 0.1026\n",
      "Epoch 10, Class Loss: 0.5435, Domain Source Loss: 0.6881, Domain Target Loss: 0.6930\n",
      "Class Acc on train_source: 0.7621, Domain Acc on train_source: 0.4965\n",
      "Class Acc on train_target: 0.8145, Domain Acc on train_target: 0.5350\n",
      "Class Acc on test: 0.1795\n"
     ]
    }
   ],
   "source": [
    "net = SpikingDANN()  # Move to GPU if available  \n",
    "  \n",
    "train1_loader = torch.utils.data.DataLoader(train1_data, batch_size=128, shuffle=True)\n",
    "train2_loader = torch.utils.data.DataLoader(train2_data, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=True)\n",
    "\n",
    "# Start training  \n",
    "train_snn(net, train1_loader, train2_loader, test_loader)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
